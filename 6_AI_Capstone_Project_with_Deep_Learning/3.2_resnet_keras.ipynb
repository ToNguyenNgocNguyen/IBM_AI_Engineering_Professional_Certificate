{"cells":[{"cell_type":"markdown","id":"77887c9d-c2ff-4dd4-b2c6-64bfa1984219","metadata":{},"source":["<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n","\n","<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"]},{"cell_type":"markdown","id":"13b30d7e-8769-497a-8ff3-410157a58567","metadata":{},"source":["## Objective\n"]},{"cell_type":"markdown","id":"373dcde3-aeb9-47f2-95b4-832192acc8de","metadata":{},"source":["In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"]},{"cell_type":"markdown","id":"cc030d9b-7972-4688-86b1-205951d1163b","metadata":{},"source":["## Table of Contents\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 3> \n","    \n","1. <a href=\"#item31\">Import Libraries and Packages</a>\n","2. <a href=\"#item32\">Download Data</a>  \n","3. <a href=\"#item33\">Define Global Constants</a>  \n","4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n","5. <a href=\"#item35\">Compile and Fit Model</a>\n","\n","</font>\n","    \n","</div>\n"]},{"cell_type":"markdown","id":"6773ce55-dd0f-49b4-aa00-0c6d6d4e10a8","metadata":{},"source":["   \n"]},{"cell_type":"markdown","id":"9d762fa9-3236-411c-af87-b8de53f576a8","metadata":{},"source":["<a id='item31'></a>\n"]},{"cell_type":"markdown","id":"1da2417d-cf35-4426-933e-0f94ab2704b1","metadata":{},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","id":"86e78465-733f-43e0-8404-fe3539697b0e","metadata":{},"source":["Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"]},{"cell_type":"code","execution_count":1,"id":"fe98db21-badd-4981-8a56-37f238670c3b","metadata":{},"outputs":[],"source":["# import skillsnetwork "]},{"cell_type":"markdown","id":"df933b81-9331-4f83-a691-f4000da40400","metadata":{},"source":["First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"]},{"cell_type":"code","execution_count":2,"id":"ae3d4906-1ca0-4da4-8e9d-0318c7cf80f3","metadata":{},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","id":"feb9d8f2-3a9b-41fe-a4bd-2eb9c0a762f3","metadata":{},"source":["In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"]},{"cell_type":"code","execution_count":3,"id":"4ac3c0af-d7d2-4b86-b360-f43231471500","metadata":{},"outputs":[],"source":["import keras\n","from tensorflow import device\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","id":"feef4bee-e4f2-4ecd-9677-1208dce1700b","metadata":{},"source":["Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"]},{"cell_type":"code","execution_count":4,"id":"0c1537cb-03a6-4f04-b906-ab9ed76d26ee","metadata":{},"outputs":[],"source":["from keras.applications.resnet import ResNet50, preprocess_input"]},{"cell_type":"markdown","id":"9cb5437b-c5e6-4967-8de1-c03c746e2ece","metadata":{},"source":["<a id='item32'></a>\n"]},{"cell_type":"markdown","id":"a5819d9d-af9c-429b-914c-64eb76e60ce8","metadata":{},"source":["## Download Data\n"]},{"cell_type":"markdown","id":"0d995f70-5dfb-47dd-a116-a11c3235ba8f","metadata":{},"source":["In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"]},{"cell_type":"code","execution_count":5,"id":"b0ae1a20-2062-4f1b-96ce-6e0674975622","metadata":{},"outputs":[],"source":["## get the data\n","# await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\", overwrite=True)"]},{"cell_type":"markdown","id":"1adf0e50-909c-4bdb-89d9-c773a89216ed","metadata":{},"source":["Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"]},{"cell_type":"markdown","id":"9d9d9beb-9dd8-4e83-9ca9-9e2c09f19eab","metadata":{},"source":["**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"]},{"cell_type":"markdown","id":"366d2205-be27-4b90-82cc-3dd130fd5fd9","metadata":{},"source":["<a id='item33'></a>\n"]},{"cell_type":"markdown","id":"a4533766-4ae0-432e-b6cd-e32b558fadf0","metadata":{},"source":["## Define Global Constants\n"]},{"cell_type":"markdown","id":"acfc926b-9b40-4ead-81aa-b6f4e3dbfd88","metadata":{},"source":["Here, we will define constants that we will be using throughout the rest of the lab. \n","\n","1. We are obviously dealing with two classes, so *num_classes* is 2. \n","2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n","3. We will training and validating the model using batches of 100 images.\n"]},{"cell_type":"code","execution_count":6,"id":"eadf578c-16b2-425a-885e-1f5202becc12","metadata":{},"outputs":[],"source":["num_classes = 2\n","\n","image_resize = 224\n","\n","batch_size_training = 100\n","batch_size_validation = 100"]},{"cell_type":"markdown","id":"ddea71c8-954b-4a80-9664-dcaa0d589811","metadata":{},"source":["<a id='item34'></a>\n"]},{"cell_type":"markdown","id":"7a4a47fa-9090-4e6e-8c78-d98b03cfc38d","metadata":{},"source":["## Construct ImageDataGenerator Instances\n"]},{"cell_type":"markdown","id":"352ef6bc-0d38-4949-808a-a37b0b051871","metadata":{},"source":["In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"]},{"cell_type":"code","execution_count":7,"id":"72a935a2-9f94-42e6-b764-e01de7ac1af9","metadata":{},"outputs":[],"source":["data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    validation_split=0.25\n",")"]},{"cell_type":"markdown","id":"0e7df7c6-d10f-466d-bc7b-738eca43d839","metadata":{},"source":["Next, we will use the *flow_from_directory* method to get the training images as follows:\n"]},{"cell_type":"code","execution_count":8,"id":"14dd3fdb-bc4e-4af5-8c3b-24292f73dcdf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 30000 images belonging to 2 classes.\n"]}],"source":["train_generator = data_generator.flow_from_directory(\n","    'resources/data',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical',\n","    subset='training')"]},{"cell_type":"markdown","id":"60d98b33-d357-4a11-ad61-63055ba10334","metadata":{},"source":["**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"]},{"cell_type":"markdown","id":"e4537a5e-adcf-4722-95c0-9b277b445bd6","metadata":{},"source":["**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"]},{"cell_type":"code","execution_count":9,"id":"3d4b940f-2dc4-44f0-b26e-a04906e0bcfb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 10000 images belonging to 2 classes.\n"]}],"source":["## Type your answer here\n","\n","validation_generator = data_generator.flow_from_directory(\n","    'resources/data',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical',\n","    subset='validation')"]},{"cell_type":"markdown","id":"83c9dbef-2907-497a-9d4a-4ad2828a563f","metadata":{},"source":["Double-click __here__ for the solution.\n","<!-- The correct answer is:\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')\n","-->\n","\n"]},{"cell_type":"markdown","id":"c5afab31-6a54-43a2-ae3f-6a85e9c5ffe8","metadata":{},"source":["<a id='item35'></a>\n"]},{"cell_type":"markdown","id":"c20de955-f27d-461c-8c71-9b0189014f71","metadata":{},"source":["## Build, Compile and Fit Model\n"]},{"cell_type":"markdown","id":"ade32f00-7084-47c7-8b46-6350be8350bb","metadata":{},"source":["In this section, we will start building our model. We will use the Sequential model class from Keras.\n"]},{"cell_type":"code","execution_count":10,"id":"e6e60cc3-76f1-4096-ab2f-2ea30f49700c","metadata":{},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"markdown","id":"0fdc30dd-813a-4b2c-b23d-e48b0f3b5530","metadata":{},"source":["Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"]},{"cell_type":"code","execution_count":11,"id":"391d3165-5d15-407d-bb54-9262b92ddffd","metadata":{},"outputs":[],"source":["model.add(ResNet50(\n","    include_top=False,\n","    pooling='avg',\n","    weights='imagenet',\n","    ))"]},{"cell_type":"markdown","id":"382b4886-4d8b-4451-baca-462437fb18e6","metadata":{},"source":["Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"]},{"cell_type":"code","execution_count":12,"id":"3d6f3e8a-b880-44cb-96f6-aaa53a23b987","metadata":{},"outputs":[],"source":["model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"markdown","id":"79deafb4-323b-4cc6-80e8-97a9d8d3703e","metadata":{},"source":["You can access the model's layers using the *layers* attribute of our model object. \n"]},{"cell_type":"code","execution_count":13,"id":"f7a33afa-9d85-47e6-8746-5bf8a6378fae","metadata":{},"outputs":[{"data":{"text/plain":["[<keras.engine.functional.Functional at 0x216065a6440>,\n"," <keras.layers.core.dense.Dense at 0x216065a08b0>]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model.layers"]},{"cell_type":"markdown","id":"934150ad-a32e-4d67-99cb-e3c8c6f63292","metadata":{},"source":["You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"]},{"cell_type":"markdown","id":"483de117-cda0-4b1e-bf37-18871f0c8977","metadata":{},"source":["You can access the ResNet50 layers by running the following:\n"]},{"cell_type":"code","execution_count":14,"id":"93ad07b7-a873-421d-a90f-9f6a8f1644d8","metadata":{},"outputs":[{"data":{"text/plain":["[<keras.engine.input_layer.InputLayer at 0x2164e6035b0>,\n"," <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x2167156e8c0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2167156eb90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2167156f1f0>,\n"," <keras.layers.core.activation.Activation at 0x2167156ef50>,\n"," <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x216716949a0>,\n"," <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x21671695cf0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671697820>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671697340>,\n"," <keras.layers.core.activation.Activation at 0x21671695a20>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671bf2800>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671bf2f50>,\n"," <keras.layers.core.activation.Activation at 0x21671bf1e40>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671696800>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671bf2050>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671696d40>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671bf1bd0>,\n"," <keras.layers.merging.add.Add at 0x2167156f910>,\n"," <keras.layers.core.activation.Activation at 0x216716940a0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671bf2b30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671bf9900>,\n"," <keras.layers.core.activation.Activation at 0x21671bf3070>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671bf9ed0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671bfb640>,\n"," <keras.layers.core.activation.Activation at 0x21671bf8100>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c08dc0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c098d0>,\n"," <keras.layers.merging.add.Add at 0x21671c08d30>,\n"," <keras.layers.core.activation.Activation at 0x21671c0aec0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c09030>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c184c0>,\n"," <keras.layers.core.activation.Activation at 0x21671c0b3a0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c0bfd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c1a200>,\n"," <keras.layers.core.activation.Activation at 0x21671c0b370>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c1bcd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c1bfa0>,\n"," <keras.layers.merging.add.Add at 0x21671c1ab00>,\n"," <keras.layers.core.activation.Activation at 0x21671c31d50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c33b20>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c33190>,\n"," <keras.layers.core.activation.Activation at 0x21671c31f60>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c56620>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c55c60>,\n"," <keras.layers.core.activation.Activation at 0x21671c54ee0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c30850>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c57400>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c33040>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c553c0>,\n"," <keras.layers.merging.add.Add at 0x21671c57550>,\n"," <keras.layers.core.activation.Activation at 0x21671c66650>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c656c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c67ac0>,\n"," <keras.layers.core.activation.Activation at 0x21671c667d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c67190>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c647f0>,\n"," <keras.layers.core.activation.Activation at 0x21671c67c10>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c577c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c196f0>,\n"," <keras.layers.merging.add.Add at 0x21671c67220>,\n"," <keras.layers.core.activation.Activation at 0x21671c091e0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c32620>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671696080>,\n"," <keras.layers.core.activation.Activation at 0x21671c19de0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671695f60>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c7e8f0>,\n"," <keras.layers.core.activation.Activation at 0x21671694760>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c7c190>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c88b80>,\n"," <keras.layers.merging.add.Add at 0x21671c7f1f0>,\n"," <keras.layers.core.activation.Activation at 0x21671c8a440>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c88be0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c8b730>,\n"," <keras.layers.core.activation.Activation at 0x21671c8a650>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c8beb0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c99510>,\n"," <keras.layers.core.activation.Activation at 0x21671c8bf10>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c9afe0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c9b760>,\n"," <keras.layers.merging.add.Add at 0x21671c99e10>,\n"," <keras.layers.core.activation.Activation at 0x21671cb8e80>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671cbae30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671cbb5e0>,\n"," <keras.layers.core.activation.Activation at 0x21671cb8ee0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671cc9090>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671cca290>,\n"," <keras.layers.core.activation.Activation at 0x21671cc9000>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671cb8730>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671ccba00>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671cba350>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671cc9540>,\n"," <keras.layers.merging.add.Add at 0x21671cca830>,\n"," <keras.layers.core.activation.Activation at 0x21671ccb9a0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671cc8d90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671cb8dc0>,\n"," <keras.layers.core.activation.Activation at 0x21671c995d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c99930>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671ccb3d0>,\n"," <keras.layers.core.activation.Activation at 0x21671cbbcd0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21671c9b550>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c30d30>,\n"," <keras.layers.merging.add.Add at 0x21671bf0760>,\n"," <keras.layers.core.activation.Activation at 0x21606526b30>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065260e0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21606527e50>,\n"," <keras.layers.core.activation.Activation at 0x21606526d40>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21606527880>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2160652dbd0>,\n"," <keras.layers.core.activation.Activation at 0x21606527940>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2160652f6a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2160652fe20>,\n"," <keras.layers.merging.add.Add at 0x2160652e4d0>,\n"," <keras.layers.core.activation.Activation at 0x21606539720>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21606538c10>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2160653aa10>,\n"," <keras.layers.core.activation.Activation at 0x21606539930>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2160653afe0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065397b0>,\n"," <keras.layers.core.activation.Activation at 0x2160653b1f0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2160655a260>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2160655a9e0>,\n"," <keras.layers.merging.add.Add at 0x21606559390>,\n"," <keras.layers.core.activation.Activation at 0x2160655b2b0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065748b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065755d0>,\n"," <keras.layers.core.activation.Activation at 0x21606558f70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065750c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21606577310>,\n"," <keras.layers.core.activation.Activation at 0x216065752d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21606588d30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065895a0>,\n"," <keras.layers.merging.add.Add at 0x21606576710>,\n"," <keras.layers.core.activation.Activation at 0x21606577700>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2160655a650>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21606558520>,\n"," <keras.layers.core.activation.Activation at 0x21606558250>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21606538700>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2160652d3c0>,\n"," <keras.layers.core.activation.Activation at 0x2160653bd60>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2160652f220>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21671c99720>,\n"," <keras.layers.merging.add.Add at 0x216065258a0>,\n"," <keras.layers.core.activation.Activation at 0x2160658bd60>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x21606589270>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x21606589240>,\n"," <keras.layers.core.activation.Activation at 0x2160658bf70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065a3d30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065a4a90>,\n"," <keras.layers.core.activation.Activation at 0x216065a2e30>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065a04f0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065a3ee0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065a1240>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065a61a0>,\n"," <keras.layers.merging.add.Add at 0x21606588b80>,\n"," <keras.layers.core.activation.Activation at 0x2160658be80>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065a7fd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065a7ca0>,\n"," <keras.layers.core.activation.Activation at 0x216065a5e70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065c2050>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065c1de0>,\n"," <keras.layers.core.activation.Activation at 0x216065c0400>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065c3670>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065c3af0>,\n"," <keras.layers.merging.add.Add at 0x216065c2800>,\n"," <keras.layers.core.activation.Activation at 0x216065a7a30>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065c3ee0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065dc490>,\n"," <keras.layers.core.activation.Activation at 0x216065c3a00>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065deb30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065dea10>,\n"," <keras.layers.core.activation.Activation at 0x216065de0b0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x216065de2c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x216065dfa90>,\n"," <keras.layers.merging.add.Add at 0x216065c2d40>,\n"," <keras.layers.core.activation.Activation at 0x216065c23b0>,\n"," <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x216065c1540>]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model.layers[0].layers"]},{"cell_type":"markdown","id":"c9b50534-db82-48ff-a966-db9c9db20b83","metadata":{},"source":["Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"]},{"cell_type":"code","execution_count":15,"id":"e9ab99d5-d322-41e7-9c67-7da3d02c0b81","metadata":{},"outputs":[],"source":["model.layers[0].trainable = False"]},{"cell_type":"markdown","id":"936f1a04-0bcc-4a29-935e-e20d23cd76cd","metadata":{},"source":["And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"]},{"cell_type":"code","execution_count":16,"id":"80e16337-42ed-4ee6-ab2c-3fa90719d4ff","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 2048)              23587712  \n","                                                                 \n"," dense (Dense)               (None, 2)                 4098      \n","                                                                 \n","=================================================================\n","Total params: 23,591,810\n","Trainable params: 4,098\n","Non-trainable params: 23,587,712\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"31c39eb0-b6c6-4f14-9c26-b861e3d071a0","metadata":{},"source":["Next we compile our model using the **adam** optimizer.\n"]},{"cell_type":"code","execution_count":17,"id":"4b9d8738-412b-4b18-afec-ff07dd316960","metadata":{},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","id":"74173ec3-81e4-41cb-84f0-047d562b195c","metadata":{},"source":["Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"]},{"cell_type":"code","execution_count":18,"id":"d0f49844-0ce6-468a-adc0-17c2d9ddbab5","metadata":{},"outputs":[],"source":["steps_per_epoch_training = len(train_generator)\n","steps_per_epoch_validation = len(validation_generator)\n","num_epochs = 2"]},{"cell_type":"markdown","id":"7f09403d-0dd1-492b-8630-86fb412f6f5a","metadata":{},"source":["Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"]},{"cell_type":"code","execution_count":19,"id":"0b0dc574-8803-44d7-b3d7-57026a9f74c0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","300/300 [==============================] - 162s 484ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.0054 - val_accuracy: 0.9991\n","Epoch 2/2\n","300/300 [==============================] - 138s 460ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.0040 - val_accuracy: 0.9990\n"]}],"source":["with device('/device:gpu:0'):\n","    fit_history = model.fit(\n","        train_generator,\n","        steps_per_epoch=steps_per_epoch_training,\n","        epochs=num_epochs,\n","        validation_data=validation_generator,\n","        validation_steps=steps_per_epoch_validation\n","    )"]},{"cell_type":"markdown","id":"8ddfe7fa-64dc-446e-90f1-4caf95352890","metadata":{},"source":["Now that the model is trained, you are ready to start using it to classify images.\n"]},{"cell_type":"markdown","id":"acff630c-cf29-45b0-be4a-9fbcfb475b42","metadata":{},"source":["Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"]},{"cell_type":"code","execution_count":20,"id":"0424beb7-0c3f-4bb3-af40-14e852aaa92f","metadata":{},"outputs":[],"source":["model.save('classifier_resnet_model.h5')"]},{"cell_type":"markdown","id":"d02a8457-6201-4efd-9f84-ac65812ba037","metadata":{},"source":["Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"]},{"cell_type":"markdown","id":"2ebb1e58-fe8f-409e-afc8-b4c9dfc97672","metadata":{},"source":["### Thank you for completing this lab!\n","\n","This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"]},{"cell_type":"markdown","id":"d42d3248-8125-42c9-9a61-4b142ef12e35","metadata":{},"source":["This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"]},{"cell_type":"markdown","id":"1df79a67-08ce-40dd-9ee4-233b19da1060","metadata":{},"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n","\n"]},{"cell_type":"markdown","id":"feb23059-dd19-4d06-b9b7-def2386544a6","metadata":{},"source":["<hr>\n","\n","Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"]}],"metadata":{"kernelspec":{"display_name":"MyPythonEnv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
